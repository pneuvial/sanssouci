---
title: "Joint Error Rate calibration"
subtitle: "Simulations for one and two-sample tests"
author: "P. Neuvial"
date: "2018-03-27"
output: html_document
vignette: >
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{Joint Error Rate calibration}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This vignettes illustrates the following points

* simulation of one- and two-sample Gaussian equi-correlated observations
* computation of test statistics by randomization
* calibration of Joint-Family-Wise Error Rate (JER) thresholds

We start with two-sample tests because we believe they are used more frequently.

```{r}
library("sansSouci")
#set.seed(0xBEEF) # for reproducibility
```

Parameters:

```{r}
m <- 1e3
n <- 124
pi0 <- 0.8
B <- 1e3+1
rho <- 0.3
SNR <- 2

alpha <- 0.25
```

We use the function `gaussianSamples` to generate Gaussian equi-correlated samples. We then use the function `calibrateJER` to perform the calibration.

## Two-sample tests

### Simulation

```{r}
sim <- gaussianSamples(m, rho, n, pi0, SNR = SNR, prob = 0.5)
str(sim)
X <- sim$X
```

We perform JER calibration using the linear template $\mathfrak{R(\lambda)}=(R_1(\lambda),$ $\ldots,R_K(\lambda)$, where for $1 \leq k \leq K$

$$R_k(\lambda) = \left\{i\in \{1, \dots , m\}\::\: \bar{\Phi}(Z_i) >  \frac{\lambda k}{m} \right\}\,$$

where $\bar{\Phi}$ is the cdf of the $\mathcal{N}(0,1)$ distribution. Note that $p_i = \bar{\Phi}(Z_i)$ is the one-sided $p$-value associated to the test statistics $Z_i$.

### Calibration

```{r}
cal <- calibrateJER(X, B = B, alpha = alpha, refFamily = "Simes")
#calB <- calibrateJER(X, B = B, alpha = alpha, refFamily = "Beta")
```

The output of the calibration is as follows

```{r}
str(cal)
#str(calB)
```

* `stat`: $m$ test statistics calculated by permutation
* `thr` : A JER-controlling family of $K$ (here $K=m$) elements
* `lambda`: the $\lambda$-calibration parameter

Because we are under positive equi-correlation, we expect $\lambda > \alpha$ 

```{r}
cal$lambda > alpha
```

### Post hoc confidence bounds

We calculate an upper confidence bound for the number of false positives among the most significant items:

```{r}
stat <- cal$stat
o <- order(stat, decreasing = TRUE)
R <- seq_along(stat)
Vbar <- curveMaxFP(stat[o], cal$thr)
```

We compare it to the true numer of false positives among the most significant items:

```{r}
H0 <- which(sim$H == 0)
V <- cumsum(o %in% H0)
```

```{r}
plot(R, Vbar, t = 's', xlab = "Number of rejections", ylab = "Bound on the number of false positives")
lines(R, V, t = 's', col = 2)
legend("topleft", c("True number of FP", "Post hoc upper bound"), col = c(2, 1), lty = rep(1, 2))
```


## One sample tests

The code is identical, except for the line to generate the observations (where we do not specify a probability of belonging to one of the two populations using the `prob` argument):

```{r}
sim <- gaussianSamples(m, rho, n, pi0, SNR = SNR)
str(sim)
X <- sim$X
```

```{r}
cal <- calibrateJER(X, B = B, alpha = alpha, refFamily = "Simes")
str(cal)
```

Again we expect $\lambda > \alpha$.

## Session information

```{r}
sessionInfo()
```

